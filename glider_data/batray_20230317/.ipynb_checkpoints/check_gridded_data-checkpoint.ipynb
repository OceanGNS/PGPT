{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c4a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import packages\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import Rbf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa9b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Set up a Dask client with increased memory limit\n",
    "# client = dask.distributed.Client(memory_limit='8GB')  # Adjust the memory limit based on your system's available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d76a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the path to the NetCDF files and load \n",
    "# Define the path to the NetCDF files\n",
    "processing_mode = 'delayed'  # or realtime\n",
    "nc_path = './' + processing_mode + '/nc/'\n",
    "file_pattern = nc_path + '*{}*.nc'.format('trajectory')\n",
    "\n",
    "# Load the NetCDF file using xarray\n",
    "filename = sorted(glob.glob(file_pattern))[0]  # Replace with your preferred method for selecting the file\n",
    "data = xr.open_dataset(filename, engine='netcdf4', decode_times=False)\n",
    "data_df = data.sortby('time').to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf41bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use a smaller subset of the data to determine appropriate parameters\n",
    "\n",
    "subset = data_df.sample(frac=0.1, random_state=123)\n",
    "X = np.column_stack((subset['lon'], subset['lat'], subset['depth']))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "Y_scaled = scaler.fit_transform(subset['temperature'].values.reshape(-1, 1))\n",
    "\n",
    "# Define the correlation scales for horizontal and vertical dimensions\n",
    "horizontal_scale = 0.1  # km\n",
    "vertical_scale = 5.0  # m\n",
    "\n",
    "# Calculate the covariance matrix between data points\n",
    "d = cdist(X_scaled, X_scaled)\n",
    "C = np.exp(-d**2 / (2 * horizontal_scale**2))\n",
    "C[np.diag_indices_from(C)] += vertical_scale**2\n",
    "\n",
    "# Calculate the weights for the optimal interpolation\n",
    "weights = np.linalg.solve(C, Y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f0a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
